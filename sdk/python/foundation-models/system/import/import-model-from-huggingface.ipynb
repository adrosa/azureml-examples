{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential, ClientSecretCredential\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "workspace_ml_client = MLClient(\n",
    "        credential,\n",
    "        #subscription_id =  \"<SUBSCRIPTION_ID>\",\n",
    "        #resource_group_name =  \"<RESOURCE_GROUP>\",\n",
    "        #workspace_name =  \"WORKSPACE_NAME>\"\n",
    "        subscription_id =  \"21d8f407-c4c4-452e-87a4-e609bfb86248\", #\"<SUBSCRIPTION_ID>\"\n",
    "        resource_group_name =  \"rg-contoso-819prod\", #\"<RESOURCE_GROUP>\",\n",
    "        workspace_name =  \"mlw-contoso-819prod\", #\"WORKSPACE_NAME>\",\n",
    ")\n",
    "\n",
    "# the models, fine tuning pipelines and environments are available in the AzureML system registry, \"azureml-preview\"\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml-preview\")\n",
    "\n",
    "experiment_name = \"summarization-news-summary\"\n",
    "\n",
    "# If you already have a gpu cluster, mention it here. Else will create a new one with the name 'gpu-cluster-big'\n",
    "compute_cluster = \"gpu-cluster-big\"\n",
    "try:\n",
    "    workspace_ml_client.compute.get(compute_cluster)\n",
    "except Exception as ex:\n",
    "    compute = AmlCompute(\n",
    "        name = compute_cluster, \n",
    "        size= \"Standard_ND40rs_v2\",\n",
    "        max_instances= 2 # For multi node training set this to an integer value more than 1\n",
    "    )\n",
    "    workspace_ml_client.compute.begin_create_or_update(compute).wait()\n",
    "\n",
    "# This is the number of GPUs in a single node of the selected 'vm_size' compute. \n",
    "# Setting this to less than the number of GPUs will result in underutilized GPUs, taking longer to train.\n",
    "# Setting this to more than the number of GPUs will result in an error.\n",
    "gpus_per_node = 2 \n",
    "\n",
    "# genrating a unique timestamp that can be used for names and versions that need to be unique\n",
    "timestamp = str(int(time.time())) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import CommandComponent, PipelineComponent, Job, Component\n",
    "from azure.ai.ml import PyTorchDistribution, Input\n",
    "\n",
    "# fetch the download_model component from the system registry\n",
    "download_model_func = registry_ml_client.components.get(name=\"download_model\", version=\"0.0.1\")\n",
    "# fetch the mlflow converter component from the system registry\n",
    "mlflow_converter_func = registry_ml_client.components.get(name=\"mlflow_converter\", version=\"0.0.1\")\n",
    "\n",
    "# define the pipeline job\n",
    "@pipeline()\n",
    "def create_pipeline():\n",
    "\n",
    "    download_model_job = download_model_func(\n",
    "        model_id = \"unitary/toxic-bert\",\n",
    "        model_source = \"Huggingface\"\n",
    "    )\n",
    "    mlflow_converter_job = mlflow_converter_func(\n",
    "        model_info = download_model_job.outputs.model_info,\n",
    "        model_path = download_model_job.outputs.model_output,\n",
    "        mlflow_flavor = \"hftransformers\",\n",
    "        task_type = \"text-classification\"\n",
    "    )\n",
    "    return {\n",
    "        \"imported_model\": mlflow_converter_job.outputs.mlflow_model_output\n",
    "    }\n",
    "\n",
    "pipeline_object = create_pipeline()\n",
    "\n",
    "# don't use cached results from previous jobs\n",
    "pipeline_object.settings.force_rerun = True\n",
    "pipeline_object.settings.default_compute  = compute_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: maroon_seed_d4d52sft59\n",
      "Web View: https://ml.azure.com/runs/maroon_seed_d4d52sft59?wsid=/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourcegroups/rg-contoso-819prod/workspaces/mlw-contoso-819prod\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2023-03-24 05:52:27Z] Submitting 1 runs, first five are: 168ccbe2:7ab353a1-4085-48e2-b9ba-7b9bd20a649d\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.8/site-packages/azure/ai/ml/operations/_job_ops_helper.py:235\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[0;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[1;32m    234\u001b[0m file_handle\u001b[39m.\u001b[39mflush()\n\u001b[0;32m--> 235\u001b[0m time\u001b[39m.\u001b[39;49msleep(_wait_before_polling(time\u001b[39m.\u001b[39;49mtime() \u001b[39m-\u001b[39;49m poll_start_time))\n\u001b[1;32m    236\u001b[0m _current_details: RunDetails \u001b[39m=\u001b[39m run_operations\u001b[39m.\u001b[39mget_run_details(job_name)  \u001b[39m# TODO use FileWatcher\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJobException\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/CODE/REPOS/azureml-examples/sdk/python/foundation-models/system/import/import-model-from-huggingface.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bmanojubuntu/mnt/c/CODE/REPOS/azureml-examples/sdk/python/foundation-models/system/import/import-model-from-huggingface.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m pipeline_job \u001b[39m=\u001b[39m workspace_ml_client\u001b[39m.\u001b[39mjobs\u001b[39m.\u001b[39mcreate_or_update(pipeline_object, experiment_name\u001b[39m=\u001b[39mexperiment_name)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bmanojubuntu/mnt/c/CODE/REPOS/azureml-examples/sdk/python/foundation-models/system/import/import-model-from-huggingface.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# wait for the pipeline job to complete\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bmanojubuntu/mnt/c/CODE/REPOS/azureml-examples/sdk/python/foundation-models/system/import/import-model-from-huggingface.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m workspace_ml_client\u001b[39m.\u001b[39;49mjobs\u001b[39m.\u001b[39;49mstream(pipeline_job\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.8/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.8/site-packages/azure/ai/ml/operations/_job_operations.py:614\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[39mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[1;32m    612\u001b[0m     \u001b[39mraise\u001b[39;00m PipelineChildJobError(job_id\u001b[39m=\u001b[39mjob_object\u001b[39m.\u001b[39mid)\n\u001b[0;32m--> 614\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream_logs_until_completion(\n\u001b[1;32m    615\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_runs_operations, job_object, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_datastore_operations, requests_pipeline\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_requests_pipeline\n\u001b[1;32m    616\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.8/site-packages/azure/ai/ml/operations/_job_ops_helper.py:313\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[0;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     error_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    308\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe output streaming for the run interrupted.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBut the run is still executing on the compute target. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDetails for canceling the run can be found here: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://aka.ms/aml-docs-cancel-run\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m     )\n\u001b[0;32m--> 313\u001b[0m     \u001b[39mraise\u001b[39;00m JobException(\n\u001b[1;32m    314\u001b[0m         message\u001b[39m=\u001b[39merror_message,\n\u001b[1;32m    315\u001b[0m         target\u001b[39m=\u001b[39mErrorTarget\u001b[39m.\u001b[39mJOB,\n\u001b[1;32m    316\u001b[0m         no_personal_data_message\u001b[39m=\u001b[39merror_message,\n\u001b[1;32m    317\u001b[0m         error_category\u001b[39m=\u001b[39mErrorCategory\u001b[39m.\u001b[39mUSER_ERROR,\n\u001b[1;32m    318\u001b[0m     )\n",
      "\u001b[0;31mJobException\u001b[0m: The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run"
     ]
    }
   ],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = workspace_ml_client.jobs.create_or_update(pipeline_object, experiment_name=experiment_name)\n",
    "# wait for the pipeline job to complete\n",
    "workspace_ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline job outputs:  {'imported_model': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f3684b8fa30>}\n",
      "path to register model:  azureml://jobs/maroon_seed_d4d52sft59/outputs/imported_model\n",
      "prepare to register model: \n",
      " description: unitary-toxic-bert imported from Huggingface\n",
      "name: unitary-toxic-bert\n",
      "path: azureml://jobs/maroon_seed_d4d52sft59/outputs/imported_model\n",
      "properties: {}\n",
      "tags: {}\n",
      "type: mlflow_model\n",
      "version: '1679638111'\n",
      "\n",
      "registered model: \n",
      " creation_context:\n",
      "  created_at: '2023-03-24T06:09:11.849047+00:00'\n",
      "  created_by: Manoj Bableshwar\n",
      "  created_by_type: User\n",
      "  last_modified_at: '2023-03-24T06:09:11.849047+00:00'\n",
      "  last_modified_by: Manoj Bableshwar\n",
      "  last_modified_by_type: User\n",
      "description: unitary-toxic-bert imported from Huggingface\n",
      "flavors:\n",
      "  hftransformers:\n",
      "    code: ''\n",
      "    hf_pretrained_class: AutoModelForSequenceClassification\n",
      "    huggingface_id: unitary/toxic-bert\n",
      "    model_data: data\n",
      "    pytorch_version: 1.11.0\n",
      "    task_type: text-classification\n",
      "    transformers_version: 4.25.1\n",
      "  python_function:\n",
      "    data: data\n",
      "    env: conda.yaml\n",
      "    loader_module: azureml.evaluate.mlflow.hftransformers\n",
      "    python_version: 3.8.16\n",
      "id: azureml:/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourceGroups/rg-contoso-819prod/providers/Microsoft.MachineLearningServices/workspaces/mlw-contoso-819prod/models/unitary-toxic-bert/versions/1679638111\n",
      "job_name: maroon_seed_d4d52sft59\n",
      "name: unitary-toxic-bert\n",
      "path: azureml://subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourceGroups/rg-contoso-819prod/workspaces/mlw-contoso-819prod/datastores/workspaceblobstore/paths/azureml/67643b41-0815-4d7a-a676-e2c76d259aeb/mlflow_model_output/\n",
      "properties: {}\n",
      "tags: {}\n",
      "type: mlflow_model\n",
      "version: '1679638111'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "# check if the `trained_model` output is available\n",
    "print (\"pipeline job outputs: \", workspace_ml_client.jobs.get(pipeline_job.name).outputs)\n",
    "\n",
    "#fetch the model from pipeline job output - not working, hence fetching from fine tune child job\n",
    "model_path_from_job = (\"azureml://jobs/{0}/outputs/{1}\".format(pipeline_job.name, \"imported_model\"))\n",
    "\n",
    "imported_model_name = \"unitary-toxic-bert\"\n",
    "\n",
    "print(\"path to register model: \", model_path_from_job)\n",
    "prepare_to_register_model = Model(\n",
    "    path=model_path_from_job,\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    "    name=imported_model_name,\n",
    "    version=timestamp, # use timestamp as version to avoid version conflict\n",
    "    description=imported_model_name + \" imported from Huggingface\"\n",
    ")\n",
    "print(\"prepare to register model: \\n\", prepare_to_register_model)\n",
    "#register the model from pipeline job output \n",
    "registered_model = workspace_ml_client.models.create_or_update(prepare_to_register_model)\n",
    "print (\"registered model: \\n\", registered_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "\n",
    "# Create online endpoint - endpoint names need to be unique in a region, hence using timestamp to create unique endpoint name\n",
    "\n",
    "online_endpoint_name = \"emotion-\" + timestamp\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Online endpoint for \" + registered_model.name + \", fine tuned model for emotion detection\",\n",
    "    auth_mode=\"key\"\n",
    ")\n",
    "workspace_ml_client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint emotion-1679638111 exists\n",
      "data_collector is not a known attribute of class <class 'azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedOnlineDeployment'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................."
     ]
    }
   ],
   "source": [
    "# create a deployment\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=\"demo1\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=registered_model.id,\n",
    "    instance_type=\"Standard_DS4_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "workspace_ml_client.online_deployments.begin_create_or_update(demo_deployment).wait()\n",
    "endpoint.traffic = {\"demo\": 100}\n",
    "workspace_ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
